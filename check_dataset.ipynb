{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# dataset = load_dataset('scene_parse_150', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "\n",
    "# image_folder = '/home/dchenbs/workspace/datasets/ADE20k/train'\n",
    "# for i in tqdm.tqdm(range(len(dataset))):\n",
    "#     image = dataset[i]['image']\n",
    "#     image_path = f'{image_folder}/{i}.jpg'\n",
    "\n",
    "#     image.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForSemanticSegmentation, AutoImageProcessor\n",
    "import torch.nn as nn\n",
    "from model.directsam import DirectSAM\n",
    "from evaluation.metrics import recall_with_tolerance\n",
    "from evaluation.visualization import compare_boundaries\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "from data.create_dataset import create_dataset\n",
    "\n",
    "dataset_configs = json.load(open('data/dataset_configs.json', 'r'))\n",
    "print(dataset_configs.keys())\n",
    "\n",
    "train_datasets = [\n",
    "    'LIP', 'CelebA', 'SOBA', 'SeginW', 'CIHP', 'Fashionpedia', 'PascalPanopticParts', 'SPIN', 'PartImageNet++', 'ADE20k', 'EntitySeg', 'LoveDA', 'COCONut-s', 'COCONut-b', 'COCONut-l', 'PACO', 'LVIS', 'COIFT', 'DIS5K-DIS-TR', 'DUTS-TR', 'ecssd', 'fss_all', 'HRSOD', 'MSRA_10K', 'ThinObject5K'\n",
    "    ]\n",
    "\n",
    "validataion_datases = [\n",
    "    'LIP', 'DRAM', 'SOBA', 'SeginW', 'CIHP', 'Fashionpedia', 'PascalPanopticParts', 'SPIN', 'PartImageNet++', 'ADE20k', 'EntitySeg', 'LoveDA', 'COCONut_relabeld_COCO_val', 'PACO', 'LVIS', 'DIS5K-DIS-VD', 'DUTS-TE'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 1024\n",
    "threshold = 0.3\n",
    "\n",
    "tolerance = resolution // 100 \n",
    "tolerance += tolerance % 2 == 0\n",
    "\n",
    "model = DirectSAM(\n",
    "    # \"chendelong/DirectSAM-1800px-0424\",\n",
    "    \"/home/dchenbs/workspace/DirectSAM/runs/directsam_pseudo_label_merged/0829-1210-1024px-from-chendelong_DirectSAM-1800px-0424/checkpoint-8000\",\n",
    "    resolution, \n",
    "    threshold,\n",
    "    device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in ['ADE20k']:\n",
    "# for dataset_name in train_datasets: \n",
    "# for dataset_name in list(dataset_configs.keys()):\n",
    "\n",
    "    dataset_config = dataset_configs[dataset_name]\n",
    "\n",
    "    dataset = create_dataset(dataset_config, split='train', resolution=resolution, thickness=2)\n",
    "\n",
    "    print(dataset_config)\n",
    "    print(dataset_name)\n",
    "    print(len(dataset))\n",
    "\n",
    "    \n",
    "    for i in tqdm.tqdm(range(10)):\n",
    "        # sample = dataset[random.randint(0, len(dataset)-1)]\n",
    "        sample = dataset[i]\n",
    "\n",
    "        # ADE20k not ready\n",
    "        image_path = dataset.image_paths[i]\n",
    "        image = Image.open(image_path)\n",
    "        print(image_path, image.size)\n",
    "\n",
    "        if type(sample) == dict:\n",
    "            image = sample['image']\n",
    "            target = sample['label']\n",
    "        else:\n",
    "            image, target = sample\n",
    "\n",
    "\n",
    "        prediction, num_tokens = model(image)\n",
    "        recall = recall_with_tolerance(target, prediction, tolerance)\n",
    "\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title('Input image')\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.imshow(compare_boundaries(target, prediction, tolerance=tolerance, linewidth=3))\n",
    "        plt.title(f'Recall: {recall:.2f}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.imshow(target, cmap='Reds')\n",
    "        plt.imshow(image, alpha=0.3)\n",
    "        plt.axis('off')\n",
    "        plt.title('Ground Truth Label')\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.imshow(prediction, cmap='Blues')\n",
    "        plt.imshow(image, alpha=0.3)\n",
    "        plt.title(f'DirectSAM Pseudo label ({num_tokens} tokens)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
