{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda:3\"\n",
    "\n",
    "from create_dataset import create_dataset\n",
    "\n",
    "dataset_configs = json.load(open('config.json', 'r'))\n",
    "print(dataset_configs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForSemanticSegmentation, AutoImageProcessor\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DirectSAM():\n",
    "\n",
    "    def __init__(self, model_name, resolution, device):\n",
    "        self.model = AutoModelForSemanticSegmentation.from_pretrained(model_name).to(device).half().eval()\n",
    "        self.processor = AutoImageProcessor.from_pretrained('chendelong/DirectSAM-1800px-0424')\n",
    "\n",
    "        self.processor.size['height'] = resolution\n",
    "        self.processor.size['width'] = resolution\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def __call__(self, image):\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.to(self.model.device).to(self.model.dtype)\n",
    "        logits = self.model(pixel_values=pixel_values).logits.float().cpu()\n",
    "\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=(self.resolution, self.resolution),\n",
    "            mode=\"bicubic\",\n",
    "        )\n",
    "        probabilities = torch.sigmoid(upsampled_logits).detach().numpy()[0,0]\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "resolution = 768\n",
    "\n",
    "thickness = 2\n",
    "\n",
    "bzp_offset = resolution // 100\n",
    "tolerance = resolution // 100 + resolution % 2\n",
    "\n",
    "n_samples = 1000\n",
    "threshold_steps = 0.01\n",
    "\n",
    "\n",
    "thresholds = np.linspace(threshold_steps, 1-threshold_steps, int(1 / threshold_steps)-1)\n",
    "\n",
    "model = DirectSAM(\n",
    "    \"chendelong/DirectSAM-tiny-distilled-15ep-768px-0821\", \n",
    "    # \"chendelong/DirectSAM-1800px-0424\",\n",
    "    resolution, \n",
    "    device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def edge_zero_padding(boundary, offset):\n",
    "    boundary[:offset, :] = 0\n",
    "    boundary[-offset:, :] = 0\n",
    "    boundary[:, :offset] = 0\n",
    "    boundary[:, -offset:] = 0\n",
    "    return boundary\n",
    "\n",
    "\n",
    "def calculate_recall_torch(target, predictions, r):\n",
    "\n",
    "    r = int(r)\n",
    "    assert r % 2 == 1 and r > 0\n",
    "    target = torch.tensor(target).to(device).float()\n",
    "    predictions = torch.tensor(predictions).to(device).float()\n",
    "\n",
    "    C, H, W = predictions.shape\n",
    "    kernel = torch.ones((C, 1, r, r)).to(device)\n",
    "\n",
    "    predictions_blur = F.conv2d(predictions, kernel, groups=C, padding=r//2)\n",
    "    overlap = target * (predictions_blur > 0)\n",
    "    recall = overlap.sum(dim=(1, 2)) / target.sum()\n",
    "\n",
    "    return recall.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_num_tokens(boundary):\n",
    "    num_objects, labels = cv2.connectedComponents((1-boundary).astype(np.uint8))\n",
    "    return num_objects\n",
    "\n",
    "\n",
    "def get_metrics(target, prob, thresholds, bzp_offset=bzp_offset, tolerance=tolerance, step=threshold_steps):\n",
    "    target = edge_zero_padding(target, bzp_offset)\n",
    "\n",
    "    predictions = np.array([prob > threshold for threshold in thresholds])\n",
    "\n",
    "    all_num_tokens = []\n",
    "    for prediction in predictions:\n",
    "        num_tokens = get_num_tokens(prediction)\n",
    "        all_num_tokens.append(num_tokens)\n",
    "\n",
    "    all_recall = calculate_recall_torch(target, predictions, tolerance)\n",
    "\n",
    "    return all_num_tokens, all_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name in ['ADE20k', 'EntitySeg', 'COCONut_relabeld_COCO_val', 'LoveDA', 'PascalPanopticParts', 'PartImageNet++']:\n",
    "for dataset_name in ['PascalPanopticParts']:\n",
    "\n",
    "    dataset_config = dataset_configs[dataset_name]\n",
    "\n",
    "    dataset = create_dataset(dataset_config, split='validation', resolution=resolution, thickness=thickness)\n",
    "\n",
    "    print(dataset_config)\n",
    "    print(dataset_name)\n",
    "    print(len(dataset))\n",
    "\n",
    "    all_num_tokens = []\n",
    "    all_recall = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(n_samples)):\n",
    "        sample = dataset[i]\n",
    "        # sample = dataset[random.randint(0, len(dataset)-1)]\n",
    "\n",
    "        if type(sample) == dict:\n",
    "            image = sample['image']\n",
    "            target = sample['label']\n",
    "        else:\n",
    "            image, target = sample\n",
    "\n",
    "        prob = model(image)\n",
    "\n",
    "        num_tokens, recall = get_metrics(target, prob, thresholds)\n",
    "        all_num_tokens.append(num_tokens)\n",
    "        all_recall.append(recall)\n",
    "\n",
    "        # if i<1:\n",
    "\n",
    "        #     plt.figure(figsize=(20, 5))\n",
    "        #     plt.subplot(1, 4, 1)\n",
    "        #     plt.imshow(image)\n",
    "            \n",
    "        #     plt.subplot(1, 4, 2)\n",
    "        #     plt.imshow(target)\n",
    "\n",
    "        #     plt.subplot(1, 4, 3)\n",
    "        #     plt.imshow(prob)\n",
    "\n",
    "        #     plt.subplot(1, 4, 4)\n",
    "        #     plt.imshow(prob > 0.5)\n",
    "\n",
    "        #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_step = 32\n",
    "max_num_tokens = 256\n",
    "\n",
    "y = np.array(all_recall).flatten()\n",
    "x = np.array(all_num_tokens).flatten()\n",
    "\n",
    "df = pd.DataFrame({'num_tokens': x, 'recall': y})\n",
    "bins = np.arange(0, max_num_tokens + bin_step, bin_step)\n",
    "df['binned_tokens'] = pd.cut(df['num_tokens'], bins)\n",
    "grouped = df.groupby('binned_tokens')['recall'].agg(['mean', 'std']).reset_index()\n",
    "print(grouped)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(x, y, color='blue', alpha=0.05, s=1)\n",
    "\n",
    "# plt.errorbar(grouped['binned_tokens'].apply(lambda x: x.mid), grouped['mean'], yerr=grouped['std'], fmt='o', color='red', capsize=5)\n",
    "\n",
    "plt.scatter(grouped['binned_tokens'].apply(lambda x: x.mid), grouped['mean'], color='red')\n",
    "plt.plot(grouped['binned_tokens'].apply(lambda x: x.mid), grouped['mean'], color='red')\n",
    "\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlim(0, max_num_tokens)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
