{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForSemanticSegmentation, AutoImageProcessor\n",
    "import torch.nn as nn\n",
    "from model.directsam import DirectSAM\n",
    "from evaluation.metrics import recall_with_tolerance\n",
    "from evaluation.visualization import compare_boundaries\n",
    "import cv2\n",
    "\n",
    "device = \"cuda:2\"\n",
    "\n",
    "from data.create_dataset import create_dataset\n",
    "\n",
    "dataset_configs = json.load(open('data/dataset_configs.json', 'r'))\n",
    "print(dataset_configs.keys())\n",
    "\n",
    "train_datasets = [\n",
    "    'LIP', 'CelebA', 'SOBA', 'SeginW', 'CIHP', 'Fashionpedia', 'PascalPanopticParts', 'SPIN', 'PartImageNet++', 'ADE20k', 'EntitySeg', 'LoveDA', 'COCONut-s', 'COCONut-b', 'COCONut-l', 'PACO', 'LVIS', 'COIFT', 'DIS5K-DIS-TR', 'DUTS-TR', 'ecssd', 'fss_all', 'HRSOD', 'MSRA_10K', 'ThinObject5K'\n",
    "    ]\n",
    "\n",
    "validataion_datases = [\n",
    "    'LIP', 'DRAM', 'SOBA', 'SeginW', 'CIHP', 'Fashionpedia', 'PascalPanopticParts', 'SPIN', 'PartImageNet++', 'ADE20k', 'EntitySeg', 'LoveDA', 'COCONut_relabeld_COCO_val', 'PACO', 'LVIS', 'DIS5K-DIS-VD', 'DUTS-TE'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_resolution = 768\n",
    "evaluation_resolution = 768\n",
    "threshold = 0.3\n",
    "\n",
    "tolerance = inference_resolution // 100 \n",
    "tolerance += tolerance % 2 == 0\n",
    "\n",
    "model = DirectSAM(\n",
    "    \"chendelong/DirectSAM-1800px-0424\",\n",
    "    # \"chendelong/DirectSAM-tiny-distilled-70ep-1024px-0920\",\n",
    "    inference_resolution, \n",
    "    threshold,\n",
    "    device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name in ['directsa_plus']:\n",
    "for dataset_name in ['directsam_pseudo_label_merged_denoised']:\n",
    "# for dataset_name in ['COCONut-l']:\n",
    "# for dataset_name in train_datasets: \n",
    "# for dataset_name in list(dataset_configs.keys()):\n",
    "\n",
    "    dataset_config = dataset_configs[dataset_name]\n",
    "\n",
    "    dataset = create_dataset(dataset_config, split='validation', resolution=evaluation_resolution, thickness=-1)\n",
    "\n",
    "    print(dataset_config)\n",
    "    print(dataset_name)\n",
    "    print(len(dataset))\n",
    "\n",
    "    all_tokens = []\n",
    "    all_recall = []\n",
    "    \n",
    "    for i in tqdm.tqdm(range(10)):\n",
    "        sample = dataset[random.randint(0, len(dataset)-1)]\n",
    "        # sample = dataset[i]\n",
    "\n",
    "        # image_path = dataset.image_paths[i]\n",
    "        # image = Image.open(image_path)\n",
    "        # print(image_path, image.size)\n",
    "\n",
    "        if type(sample) == dict:\n",
    "            image = sample['image']\n",
    "            target = sample['label']\n",
    "        else:\n",
    "            image, target = sample\n",
    "\n",
    "        prediction, num_tokens = model(image, post_processing=True)\n",
    "        prediction = cv2.resize(prediction.astype(np.float32), (evaluation_resolution, evaluation_resolution), interpolation=cv2.INTER_NEAREST).astype(np.uint8)\n",
    "        # recall = recall_with_tolerance(target, prediction, tolerance)\n",
    "        recall = 0\n",
    "        print(f\"Recall: {recall:.4f}\\tNumber of tokens: {num_tokens}\")\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title('Input image')\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.imshow(compare_boundaries(target, prediction, tolerance=tolerance, linewidth=3))\n",
    "        plt.title(f'Recall: {recall:.2f}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.imshow(target, cmap='Reds')\n",
    "        plt.imshow(image, alpha=0.3)\n",
    "        plt.axis('off')\n",
    "        plt.title('Ground Truth Label')\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.imshow(prediction, cmap='Blues')\n",
    "        plt.imshow(image, alpha=0.3)\n",
    "        plt.title(f'DirectSAM Pseudo label ({num_tokens} tokens)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        all_tokens.append(num_tokens)\n",
    "        all_recall.append(recall)\n",
    "\n",
    "    print(f\"Average number of tokens: {np.mean(all_tokens):.2f}\")\n",
    "    print(f\"Average recall: {np.mean(all_recall):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
